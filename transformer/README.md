# Transformer Components

PyTorch transformer encoder implementation with causal masking support for both regression and autoregressive tasks.

## Features

- **Multi-Head Attention**: Scaled dot-product attention with optional causal masking
- **Causal Masking**: Support for autoregressive generation with future token masking
- **Feedforward Network**: Two-layer MLP with ReLU activation
- **Positional Encoding**: Sinusoidal position embeddings
- **Layer Normalization**: Post-norm transformer architecture
- **Dual Architecture**: Regression (pooled) and autoregressive (sequence) modes

## Components

- **`transformer_model.py`** - Complete transformer models (regression + autoregressive)
- **`attention.py`** - Multi-head self-attention with causal mask support
- **`encoder.py`** - Transformer encoder layer
- **`ffn.py`** - Feedforward network
- **`input_encodings.py`** - Positional encoding

## Architecture

### Regression Mode (TransformerModel)
```
Input: [batch, seq_len, input_dim]
├── Linear Projection → [batch, seq_len, embed_dim]
├── Positional Encoding
├── Transformer Layers (N layers)
│   ├── Multi-Head Attention + Residual
│   ├── Layer Norm
│   ├── Feedforward Network + Residual
│   └── Layer Norm
├── Global Average Pooling → [batch, embed_dim]
└── Output Projection → [batch, output_dim]
```

### Autoregressive Mode (AutoregressiveTransformerModel)
```
Input: [batch, seq_len, input_dim]
├── Linear Projection → [batch, seq_len, embed_dim]
├── Positional Encoding
├── Transformer Layers (N layers) with Causal Masking
│   ├── Causal Multi-Head Attention + Residual
│   ├── Layer Norm
│   ├── Feedforward Network + Residual
│   └── Layer Norm
└── Output Projection → [batch, seq_len, output_dim]
```

## Usage

### Regression Model
```python
from transformer.transformer_model import TransformerModel

model = TransformerModel(
    input_dim=8,
    embed_dim=32,
    ffn_latent_dim=128,
    num_layers=2,
    num_heads=2,
    output_dim=1,
    apply_causal_mask=False,
    max_seq_len=128
)

# Input: [batch_size, sequence_length, input_dim]
x = torch.randn(32, 16, 8)
output = model(x)  # [32, 1] - scalar prediction
```

### Autoregressive Model
```python
from transformer.transformer_model import AutoregressiveTransformerModel

model = AutoregressiveTransformerModel(
    input_dim=1,
    embed_dim=64,
    ffn_latent_dim=128,
    num_layers=2,
    num_heads=2,
    output_dim=1,
    apply_causal_mask=True,
    max_seq_len=128
)

# Input: [batch_size, sequence_length, input_dim]
x = torch.randn(32, 16, 1)
output = model(x)  # [32, 16, 1] - full sequence

# Next token generation
next_token = model.generate_next_token(x)  # [32, 1, 1]
```

### Causal Masking
```python
from transformer.attention import create_causal_mask, scaled_dot_product_attention

# Create causal mask for sequence length 10
mask = create_causal_mask(10)  # Lower triangular matrix

# Use in attention computation
attention_output = scaled_dot_product_attention(
    query, key, value, mask=mask
)
```

## Configuration

| Parameter | Description |
|-----------|-------------|
| `input_dim` | Input feature dimension |
| `embed_dim` | Embedding/hidden dimension |
| `ffn_latent_dim` | Feedforward network hidden size |
| `num_layers` | Number of transformer layers |
| `num_heads` | Number of attention heads |
| `output_dim` | Output dimension |
| `apply_causal_mask` | Enable causal masking for autoregressive tasks |
| `max_seq_len` | Maximum sequence length for positional encoding |

## Integration

- **Regression Tasks**: Via `regression/h_transformer.py` wrapper for sequence-to-scalar prediction
- **Autoregressive Tasks**: Via `regression/h_transformer.py` for next-token prediction and generation
- **Experiment Framework**: Integrated with `regression/experiment.py` for both modes

## Key Differences

| Feature | Regression Model | Autoregressive Model |
|---------|------------------|----------------------|
| **Output** | Scalar (pooled) | Full sequence |
| **Masking** | None | Causal masking |
| **Use Case** | Classification/Regression | Sequence generation |
| **Pooling** | Global average | None |
| **Training** | Supervised learning | Next-token prediction |