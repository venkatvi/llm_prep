# Transformer Components

PyTorch transformer implementation with complete encoder, decoder, and encoder-decoder architectures supporting regression, autoregressive, and sequence-to-sequence tasks.

## Features

- **Multi-Head Attention**: Scaled dot-product attention with optional causal masking
- **Multi-Query Attention (MQA)**: Memory-efficient attention with single key/value heads
- **Group Query Attention (GQA)**: Balanced attention mechanism grouping heads for efficiency
- **Cross-Attention**: Encoder-decoder attention for sequence-to-sequence modeling
- **KV Caching**: Optimized inference with key-value caching for autoregressive generation
- **Causal Masking**: Support for autoregressive generation with future token masking
- **Feedforward Network**: Two-layer MLP with ReLU activation
- **Positional Encoding**: Sinusoidal position embeddings
- **Layer Normalization**: Post-norm transformer architecture
- **Multiple Architectures**: Regression (pooled), autoregressive (sequence), and encoder-decoder modes
- **Attention Factory**: Unified interface for different attention mechanisms
- **Type Annotations**: Comprehensive type hints for all components

## Components

- **`transformer_model.py`** - Complete transformer models (regression + autoregressive + encoder-decoder)
- **`decoder.py`** - Transformer decoder layer with cross-attention support
- **`encoder.py`** - Transformer encoder layer
- **`attention/`** - Attention mechanism implementations:
  - **`mha.py`** - Multi-Head Attention (standard transformer attention)
  - **`mqa.py`** - Multi-Query Attention (single K/V heads for efficiency)
  - **`gqa.py`** - Group Query Attention (grouped K/V heads for balance)
- **`attention_utils.py`** - Attention factory and utilities
- **`ffn.py`** - Feedforward network
- **`input_encodings.py`** - Positional encoding

## Architecture

### Regression Mode (TransformerModel)
```
Input: [batch, seq_len, input_dim]
â”œâ”€â”€ Linear Projection â†’ [batch, seq_len, embed_dim]
â”œâ”€â”€ Positional Encoding
â”œâ”€â”€ Transformer Layers (N layers)
â”‚   â”œâ”€â”€ Multi-Head Attention + Residual
â”‚   â”œâ”€â”€ Layer Norm
â”‚   â”œâ”€â”€ Feedforward Network + Residual
â”‚   â””â”€â”€ Layer Norm
â”œâ”€â”€ Global Average Pooling â†’ [batch, embed_dim]
â””â”€â”€ Output Projection â†’ [batch, output_dim]
```

### Autoregressive Mode (AutoregressiveTransformerModel)
```
Input: [batch, seq_len, input_dim]
â”œâ”€â”€ Linear Projection â†’ [batch, seq_len, embed_dim]
â”œâ”€â”€ Positional Encoding
â”œâ”€â”€ Transformer Layers (N layers) with Causal Masking
â”‚   â”œâ”€â”€ Causal Multi-Head Attention + Residual
â”‚   â”œâ”€â”€ Layer Norm
â”‚   â”œâ”€â”€ Feedforward Network + Residual
â”‚   â””â”€â”€ Layer Norm
â””â”€â”€ Output Projection â†’ [batch, seq_len, output_dim]
```

### Encoder-Decoder Mode (EncoderDecoder)
```
Source: [batch, src_len, input_dim]    Target: [batch, tgt_len, input_dim]
â”‚                                      â”‚
â”œâ”€â”€ Encoder                           â”œâ”€â”€ Decoder
â”‚   â”œâ”€â”€ Linear Projection             â”‚   â”œâ”€â”€ Linear Projection
â”‚   â”œâ”€â”€ Positional Encoding          â”‚   â”œâ”€â”€ Positional Encoding
â”‚   â””â”€â”€ Encoder Layers (N layers)    â”‚   â””â”€â”€ Decoder Layers (M layers)
â”‚       â”œâ”€â”€ Multi-Head Attention     â”‚       â”œâ”€â”€ Causal Self-Attention
â”‚       â”œâ”€â”€ Layer Norm               â”‚       â”œâ”€â”€ Layer Norm
â”‚       â”œâ”€â”€ FFN + Residual           â”‚       â”œâ”€â”€ Cross-Attention (to encoder)
â”‚       â””â”€â”€ Layer Norm               â”‚       â”œâ”€â”€ Layer Norm
â”‚                                     â”‚       â”œâ”€â”€ FFN + Residual
â””â”€â”€ Encoder Output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€ Layer Norm
    [batch, src_len, embed_dim]               â”‚
                                             â””â”€â”€ Output Projection
                                                 [batch, tgt_len, output_dim]
```

## Usage

### Regression Model
```python
from transformer.transformer_model import TransformerModel

model = TransformerModel(
    input_dim=8,
    embed_dim=32,
    ffn_latent_dim=128,
    num_layers=2,
    num_heads=2,
    output_dim=1,
    apply_causal_mask=False,
    max_seq_len=128,
    attention_type="mha"  # "mha", "mqa", or "gqa"
)

# Input: [batch_size, sequence_length, input_dim]
x = torch.randn(32, 16, 8)
output = model(x)  # [32, 1] - scalar prediction
```

### Autoregressive Model
```python
from transformer.transformer_model import AutoregressiveTransformerModel

model = AutoregressiveTransformerModel(
    input_dim=1,
    embed_dim=64,
    ffn_latent_dim=128,
    num_layers=2,
    num_heads=2,
    output_dim=1,
    apply_causal_mask=True,
    max_seq_len=128
)

# Input: [batch_size, sequence_length, input_dim]
x = torch.randn(32, 16, 1)
output = model(x)  # [32, 16, 1] - full sequence

# Next token generation
next_token = model.generate_next_token(x)  # [32, 1, 1]
```

### Encoder-Decoder Model
```python
from transformer.transformer_model import EncoderDecoder

model = EncoderDecoder(
    input_dim=1,
    embed_dim=64,
    ffn_latent_dim=128,
    num_encoder_layers=2,
    num_decoder_layers=2,
    num_heads=2,
    output_dim=1,
    apply_causal_mask=True,
    max_seq_len=128
)

# Source and target sequences
source = torch.randn(32, 16, 1)      # [batch, src_len, input_dim]
target = torch.randn(32, 20, 1)      # [batch, tgt_len, input_dim]

# Encode source sequence
encoder_output = model.encode(source)  # [32, 16, 64]

# Decode with target (teacher forcing during training)
decoder_output = model.decode(target, encoder_output)  # [32, 20, 1]

# Full forward pass
output = model(source, target)  # [32, 20, 1]
```

### Causal Masking
```python
from transformer.attention import create_causal_mask, scaled_dot_product_attention

# Create causal mask for sequence length 10
mask = create_causal_mask(10)  # Lower triangular matrix

# Use in attention computation
attention_output = scaled_dot_product_attention(
    query, key, value, mask=mask
)
```

## Configuration

| Parameter | Description |
|-----------|-------------|
| `input_dim` | Input feature dimension |
| `embed_dim` | Embedding/hidden dimension |
| `ffn_latent_dim` | Feedforward network hidden size |
| `num_layers` | Number of transformer layers (encoder-only) |
| `num_encoder_layers` | Number of encoder layers (encoder-decoder) |
| `num_decoder_layers` | Number of decoder layers (encoder-decoder) |
| `num_heads` | Number of attention heads |
| `output_dim` | Output dimension |
| `apply_causal_mask` | Enable causal masking for autoregressive tasks |
| `max_seq_len` | Maximum sequence length for positional encoding |
| `attention_type` | Type of attention mechanism ("mha", "mqa", "gqa") |
| `use_kv_cache` | Enable key-value caching for optimized inference |

## KV Caching

Key-Value caching dramatically improves inference performance for autoregressive generation by reusing previously computed key and value tensors, avoiding redundant computation across generation steps.

### How KV Caching Works

During autoregressive generation, at each step:
1. **First Pass**: Compute and cache all key/value pairs for the input sequence
2. **Subsequent Passes**: Reuse cached K,V pairs and only compute new ones for the current token
3. **Context Management**: Handle cache growth with expanding or sliding window strategies

### Usage

#### Basic KV Caching
```python
from transformer.transformer_model import AutoregressiveTransformerModel

# Enable KV caching in model configuration
model = AutoregressiveTransformerModel(
    input_dim=1, embed_dim=64, ffn_latent_dim=128,
    num_layers=2, num_heads=4, output_dim=1,
    apply_causal_mask=True, max_seq_len=128,
    attention_type="gqa", use_kv_cache=True
)

# First forward pass - initializes and populates cache
initial_sequence = torch.randn(1, 10, 1)  # [batch, seq_len, input_dim]
output1 = model(initial_sequence, expanding_context=True)

# Subsequent passes - reuses cache for efficiency
next_token = torch.randn(1, 1, 1)          # [batch, 1, input_dim] 
output2 = model(next_token, expanding_context=True)  # Much faster
```

#### Context Management Modes
```python
# Expanding Context - Cache grows with each generation step
# Good for: Short sequences, maximum context retention
model(tokens, expanding_context=True)

# Sliding Window - Fixed cache size, removes oldest entries  
# Good for: Long sequences, memory-constrained environments
model(tokens, expanding_context=False)
```

#### Encoder-Decoder KV Caching
```python
from transformer.transformer_model import EncoderDecoder

model = EncoderDecoder(
    input_dim=1, embed_dim=64, ffn_latent_dim=128,
    num_encoder_layers=2, num_decoder_layers=2, num_heads=4, output_dim=1,
    apply_causal_mask=True, max_seq_len=128,
    attention_type="mqa", use_kv_cache=True
)

# Encoder - can cache for repeated encoding of same source
source = torch.randn(1, 20, 1)
encoder_output = model.encode(source, expanding_context=False)

# Decoder - caches both self-attention and cross-attention K,V pairs
target = torch.randn(1, 1, 1)  # Start with single token
for step in range(10):
    # Each step reuses previous K,V and only computes new ones
    output = model.decode(target, encoder_output, expanding_context=True)
    # Use output to get next token for autoregressive generation
    target = torch.cat([target, output[:, -1:, :]], dim=1)
```

### Performance Benefits

| Attention Type | Memory Reduction | Speed Improvement | Cache Efficiency |
|----------------|------------------|-------------------|------------------|
| **MHA + Cache** | 40-60% | 5-8x faster | Good |
| **GQA + Cache** | 50-70% | 7-10x faster | Better |  
| **MQA + Cache** | 60-80% | 8-12x faster | Best |

### Implementation Features

- **Automatic Management**: Cache creation, updates, and cleanup handled automatically
- **Memory Efficient**: Supports both expanding and sliding window cache strategies
- **Attention Agnostic**: Works seamlessly with MHA, MQA, and GQA attention mechanisms
- **Batch Compatible**: Supports batched inference with independent caches per sample
- **Gradient Safe**: Caching preserves gradient computation during training
- **Thread Safe**: Safe for concurrent inference scenarios

### Cache Lifecycle

1. **Initialization**: Cache is created on first forward pass with `use_kv_cache=True`
2. **Population**: Key and value tensors are computed and stored
3. **Reuse**: Subsequent forward passes reuse cached values and append new ones
4. **Management**: Cache size managed based on `expanding_context` parameter
5. **Cleanup**: Cache is automatically cleared when model is reset or destroyed

### Memory Usage

For sequence length S, embed dimension D, H heads, G groups:

| Mechanism | Cache Memory per Layer |
|-----------|------------------------|
| **MHA** | 2 Ã— H Ã— S Ã— (D/H) = 2 Ã— S Ã— D |
| **GQA** | 2 Ã— G Ã— S Ã— (D/H) = 2 Ã— S Ã— D Ã— (G/H) |
| **MQA** | 2 Ã— 1 Ã— S Ã— (D/H) = 2 Ã— S Ã— (D/H) |

## Attention Mechanisms

The transformer supports three types of attention mechanisms, each with different efficiency characteristics:

### Multi-Head Attention (MHA)
```python
from transformer.attention.mha import MultiHeadAttention

attn = MultiHeadAttention(embed_dim=64, num_heads=8, apply_causal_mask=False)
```

**Standard transformer attention with multiple independent query, key, and value heads.**
- âœ… Maximum representational capacity
- âŒ Highest memory usage during inference
- âŒ Slowest inference speed
- ğŸ¯ Best for: Small models, maximum quality requirements

### Multi-Query Attention (MQA)
```python
from transformer.attention.mqa import MultiQueryAttention

attn = MultiQueryAttention(embed_dim=64, num_heads=8)
```

**Memory-efficient attention with multiple query heads but single key/value heads.**
- âœ… Lowest memory usage during inference
- âœ… Fastest inference speed
- âŒ Slight quality reduction compared to MHA
- ğŸ¯ Best for: Large models, inference-heavy applications

### Group Query Attention (GQA)
```python
from transformer.attention.gqa import GroupQueryAttention

attn = GroupQueryAttention(embed_dim=64, num_heads=8, num_groups=4)
```

**Balanced approach that groups query heads to share key/value heads.**
- âš–ï¸ Balanced memory usage and quality
- âš–ï¸ Medium inference speed
- âœ… Good compromise between MHA and MQA
- ğŸ¯ Best for: Production models requiring balance

### Attention Factory
```python
from transformer.attention_utils import get_attention

# Unified interface for all attention types
mha = get_attention("mha", embed_dim=64, num_heads=8, num_groups=4, apply_causal_mask=False, use_kv_cache=True)
mqa = get_attention("mqa", embed_dim=64, num_heads=8, num_groups=4, apply_causal_mask=False, use_kv_cache=True)
gqa = get_attention("gqa", embed_dim=64, num_heads=8, num_groups=4, apply_causal_mask=False, use_kv_cache=True)
```

### Performance Comparison

| Attention Type | Parameters | Memory (Inference) | Speed | Quality | Use Case |
|----------------|------------|-------------------|--------|---------|----------|
| **MHA** | Highest | Highest | Slowest | Best | Small models, research |
| **GQA** | Medium | Medium | Medium | Good | Production systems |
| **MQA** | Lowest | Lowest | Fastest | Good | Large models, inference |

## Integration

- **Regression Tasks**: Via `regression/h_transformer.py` wrapper for sequence-to-scalar prediction
- **Autoregressive Tasks**: Via `regression/h_transformer.py` for next-token prediction and generation
- **Sequence-to-Sequence Tasks**: Via `regression/h_transformer.py` EncoderDecoderWrapper for translation tasks
- **Experiment Framework**: Integrated with `regression/experiment.py` for all modes (regression, autoregressive, encoder-decoder)

## Key Differences

| Feature | Regression Model | Autoregressive Model | Encoder-Decoder Model |
|---------|------------------|----------------------|----------------------|
| **Output** | Scalar (pooled) | Full sequence | Target sequence |
| **Masking** | None | Causal masking | Causal in decoder |
| **Use Case** | Classification/Regression | Sequence generation | Seq2seq translation |
| **Pooling** | Global average | None | None |
| **Training** | Supervised learning | Next-token prediction | Teacher forcing |
| **Architecture** | Encoder only | Encoder only | Encoder + Decoder |
| **Cross-Attention** | No | No | Yes (decoder to encoder) |